{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Import**"
      ],
      "metadata": {
        "id": "5YJFk28rP_Bk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn.datasets as dt\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.model_selection as ms\n",
        "import sklearn.preprocessing as pp\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_breast_cancer,make_classification,make_blobs,make_circles,load_digits\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import  mean_squared_error\n",
        "from sklearn.utils import resample\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "from sklearn.linear_model import Perceptron\n",
        "from graphviz import Digraph, nohtml\n",
        "import io\n",
        "from scipy import stats\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.utils import shuffle\n",
        "from scipy.io import loadmat\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from matplotlib import cm\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.base import clone\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import statsmodels.api as sm\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "!pip install scipy\n",
        "from matplotlib.patches import Polygon\n",
        "import scipy.io as sio\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "from scipy.stats import skew\n",
        "from typing_extensions import final\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
        "from graphviz import Digraph, nohtml"
      ],
      "metadata": {
        "id": "6uAaO4E_P4en"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sigmoid**"
      ],
      "metadata": {
        "id": "x5qA09JzQGCe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKwOFnYk5IaH"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neuron**"
      ],
      "metadata": {
        "id": "WS93A4S9QkYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Neuron:\n",
        "\n",
        "    def __init__(self, in_features, af=None, loss_fn=mse, n_iter=100, eta=0.1, verbose=True):\n",
        "        self.in_features = in_features\n",
        "        # weight & bias\n",
        "        self.w = np.random.randn(in_features, 1)\n",
        "        self.b = np.random.randn()\n",
        "        self.af = af\n",
        "        self.loss_fn = loss_fn\n",
        "        self.loss_hist = []\n",
        "        self.w_grad, self.b_grad = None, None\n",
        "        self.n_iter = n_iter\n",
        "        self.eta = eta\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def predict(self, x):\n",
        "        # x: [n_samples, in_features]\n",
        "        y_hat = x @ self.w + self.b\n",
        "        y_hat = y_hat if self.af is None else self.af(y_hat)\n",
        "        return y_hat\n",
        "\n",
        "    def fit(self, x, y):\n",
        "        for i in range(self.n_iter):\n",
        "            #model\n",
        "            y_hat = self.predict(x)\n",
        "            #loss\n",
        "            loss = self.loss_fn(y, y_hat)\n",
        "            self.loss_hist.append(loss)\n",
        "            #grad\n",
        "            self.gradient(x, y, y_hat)\n",
        "            #optimize\n",
        "            self.gradient_descent()\n",
        "            #print results\n",
        "            if self.verbose & (i % 10 == 0):\n",
        "                print(f'Iter={i}, Loss={loss:.4}')\n",
        "\n",
        "    def gradient(self, x, y, y_hat):\n",
        "        self.w_grad = (x.T @ (y_hat - y)) / len(y)\n",
        "        self.b_grad = (y_hat - y).mean()\n",
        "\n",
        "    def gradient_descent(self):\n",
        "        self.w -= self.eta * self.w_grad\n",
        "        self.b -= self.eta * self.b_grad\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'Neuron({self.in_features}, {self.af.__name__})'\n",
        "\n",
        "    def parameters(self):\n",
        "        return {'w': self.w, 'b': self.b}"
      ],
      "metadata": {
        "id": "ynLY4WjfQIrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neuron = Neuron(2, af=sigmoid, loss_fn=bce, n_iter=500)\n",
        "neuron.fit(X, y[:, None])"
      ],
      "metadata": {
        "id": "GWVNKzEwQtL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4**"
      ],
      "metadata": {
        "id": "AXEbYduiWF3H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "import data"
      ],
      "metadata": {
        "id": "5RWFajctWLuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "Xu1W4wWKWWxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --no-cache-dir gdown"
      ],
      "metadata": {
        "id": "4aSJuUyqYdUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "https://drive.google.com/file/d/1_lfDMfP-0PKeb6OUqA6iHJozJuXrlGDQ/view?usp=sharing"
      ],
      "metadata": {
        "id": "DH8KsbHgXTzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1_lfDMfP-0PKeb6OUqA6iHJozJuXrlGDQ"
      ],
      "metadata": {
        "id": "Oxgyro4BZXIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df ='DATA.mat'"
      ],
      "metadata": {
        "id": "X45d7F3GZdDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_NOV9='NOV9.mat'\n",
        "df_NOV17='NOV17.mat'"
      ],
      "metadata": {
        "id": "oCTN7LAvatTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.io as sio\n",
        "data_NOV9= scipy.io.loadmat(df)\n",
        "data_NOV17= scipy.io.loadmat(df)\n"
      ],
      "metadata": {
        "id": "mEEIwv-ybpEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_NOV17"
      ],
      "metadata": {
        "id": "nAHv0ddRo0y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_NOV9"
      ],
      "metadata": {
        "id": "bzV-jNRJo4gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_NOV9_arr=np.array(data_NOV9)"
      ],
      "metadata": {
        "id": "5Y1TvmSJ-Ky3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_NOV9_arr"
      ],
      "metadata": {
        "id": "sUD9GU-N-9ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into features (X) and labels (y)\n",
        "x = data_NOV9_arr.drop('label', axis=1).values\n",
        "y = data_NOV9_arr ['label'].values\n",
        "\n",
        "# Split data into training and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5, random_state=21)\n",
        "\n",
        "# Further split test set into validation and final test sets\n",
        "x_val, x_final_test, y_val, y_final_test = train_test_split(x_test, y_test, test_size=0.5, random_state=21)\n",
        "\n",
        "# Standardize data\n",
        "scaler = StandardScaler()\n",
        "x_train_scaled = scaler.fit_transform(x_train)\n",
        "x_val_scaled = scaler.transform(x_val)\n",
        "x_final_test_scaled = scaler.transform(x_final_test)"
      ],
      "metadata": {
        "id": "I2nFrNlm_ON_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c_data_NOV9 = list(data_NOV9.keys())[:]\n",
        "c_data_NOV17 = list(data_NOV17.keys())[:]"
      ],
      "metadata": {
        "id": "AwFJ-eyoo615"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all= np.concatenate((c_data_NOV9,c_data_NOV17 ))\n",
        "all"
      ],
      "metadata": {
        "id": "-sY3oLbkpQwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mat= {}\n",
        "M, N = 200, 200\n",
        "\n",
        "for label, datasets in {'NOV9':data_NOV9}.items():\n",
        "    if label == 'NOV9':\n",
        "        cols = c_data_NOV9\n",
        "    else:\n",
        "        continue  # Skip if label is not recognized\n",
        "\n",
        "    # for col in cols:\n",
        "    #     mat1 = np.zeros((M, N))\n",
        "    #     for j in range(M):\n",
        "    #         if j + N <= len(datasets[col]):\n",
        "    #             mat1[j, :] = datasets[col][j:j+N].reshape(-1,)\n",
        "    #         else:\n",
        "    #             break  # Exit the loop if the window exceeds bounds of data\n",
        "        mat[f\"{label}_{col}\"] =mat1"
      ],
      "metadata": {
        "id": "cKKdH7U6r77A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col1 = 'Actuator'\n",
        "col2 = 'Variable symbol'\n",
        "col3 ='Variable Description'\n",
        "col4 ='Range'\n",
        "col5='Units'\n",
        "\n"
      ],
      "metadata": {
        "id": "QGVZJXbZqAZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NOV9\n",
        " Actuator Variable Symbol Variable Description Range"
      ],
      "metadata": {
        "id": "-4uYLX_ijzDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " Item Fault tag Sample\n",
        " Date\n",
        " Fault description"
      ],
      "metadata": {
        "id": "C0f0MMSkkTwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature extraction**"
      ],
      "metadata": {
        "id": "nYEtGTcoamtP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5k0dmklOjSgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define feature extraction functions\n",
        "def calculate_standard_deviation(data):\n",
        "    return np.std(data, axis=0)\n",
        "\n",
        "def find_peak(data):\n",
        "    return np.max(data, axis=0)\n",
        "\n",
        "def calculate_crest_factor(data):\n",
        "    peak_value = find_peak(data)\n",
        "    rms_value = np.sqrt(np.mean(data ** 2, axis=0))\n",
        "    return peak_value / rms_value\n",
        "\n",
        "def calculate_skewness(data):\n",
        "    return skew(data, axis=0)\n",
        "\n",
        "def calculate_clearance_factor(data):\n",
        "    peak_to_peak = np.ptp(data, axis=0)\n",
        "    rms_value = np.sqrt(np.mean(data ** 2, axis=0))\n",
        "    return peak_to_peak / rms_value\n",
        "\n",
        "def calculate_peak_to_peak(data):\n",
        "    return np.ptp(data, axis=0)\n",
        "\n",
        "def calculate_shape_factor(data):\n",
        "    mean_value = np.mean(data, axis=0)\n",
        "    std_value = np.std(data, axis=0)\n",
        "    return std_value / mean_value\n",
        "\n",
        "def calculate_impact_factor(data):\n",
        "    peak_value = find_peak(data)\n",
        "    mean_value = np.mean(data, axis=0)\n",
        "    return peak_value / mean_value"
      ],
      "metadata": {
        "id": "0IrSoGwaaqos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Features:\n",
        "    def __init__(self, matrix):\n",
        "        self.matrix = matrix\n",
        "        self._extract()\n",
        "\n",
        "    def _extract(self):\n",
        "        self.features = {\n",
        "            'standard deviation': stats.tstd(self.matrix, axis=1),\n",
        "            'peak': np.max(self.matrix, axis=1),\n",
        "            'skewness': stats.skew(self.matrix, axis=1),\n",
        "            'square root mean': np.square(np.mean(np.sqrt(np.abs(self.matrix)), axis=1)),\n",
        "            'kurtosis': stats.kurtosis(self.matrix, axis=1),\n",
        "            'crest factor': np.max(self.matrix, axis=1) / np.sqrt(np.mean(np.square(self.matrix), axis=1)),\n",
        "            'clearance factor': np.max(self.matrix, axis=1) / np.square(np.mean(np.sqrt(np.abs(self.matrix)), axis=1)),\n",
        "            'mean': np.mean(self.matrix, axis=1),\n",
        "            'absolute mean': np.mean(np.abs(self.matrix), axis=1),\n",
        "            'root mean square': np.sqrt(np.mean(np.square(self.matrix), axis=1)),\n",
        "        }\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        return self.features[key]\n",
        "\n",
        "\n",
        "# normal_features = Features(x_normal)\n",
        "# fault1_features = Features(x_fault1)\n",
        "# fault2_features = Features(x_fault2)\n",
        "# fault3_features = Features(x_fault3)\n"
      ],
      "metadata": {
        "id": "yJMhtOP1azZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_values = np.array(list(data_NOV9.values()))"
      ],
      "metadata": {
        "id": "ed-DzZgG5Jv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Features()"
      ],
      "metadata": {
        "id": "pWPlOVXZtJ9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into features (X) and labels (y)\n",
        "x = final_dataset .drop('label', axis=1).values\n",
        "y = final_dataset ['label'].values\n",
        "\n",
        "# Split data into training and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5, random_state=21)\n",
        "\n",
        "# Further split test set into validation and final test sets\n",
        "x_val, x_final_test, y_val, y_final_test = train_test_split(x_test, y_test, test_size=0.5, random_state=21)\n",
        "\n",
        "# Standardize data\n",
        "scaler = StandardScaler()\n",
        "x_train_scaled = scaler.fit_transform(x_train)\n",
        "x_val_scaled = scaler.transform(x_val)\n",
        "x_final_test_scaled = scaler.transform(x_final_test)"
      ],
      "metadata": {
        "id": "W88UXIx9nlMM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}